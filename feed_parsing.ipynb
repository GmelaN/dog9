{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Proof-of-Conecpt notepad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## fetching RSS feed by feedparser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "politics\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import json\n",
    "from datetime import datetime, timezone, timedelta\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "from time import sleep\n",
    "from random import randint\n",
    "from collections import namedtuple\n",
    "\n",
    "NOW: str = datetime.now(tz=timezone(timedelta(hours=9))).strftime(\"%Y%m%d%H%M\")\n",
    "sections = {\n",
    "    \"politics\" : 100,\n",
    "    \"economy\": 101,\n",
    "    \"social\": 102,\n",
    "    \"life\": 103,\n",
    "    \"world\": 104,\n",
    "    \"science\": 105 \n",
    "}\n",
    "\n",
    "Article = namedtuple(\"Article\", [\"title\", \"content\", \"url\", \"pub_time\", \"section\", \"press\"])\n",
    "\n",
    "articles: list[str | Article] = [NOW]\n",
    "\n",
    "header: dict = {\"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/128.0.0.0 Safari/537.36\"}\n",
    "\n",
    "for section in sections.keys():\n",
    "    for page in range(1):\n",
    "        url = f\"https://news.naver.com/section/template/SECTION_ARTICLE_LIST?sid={sections[section]}&sid2=&cluid=&pageNo={page}&date=&next={NOW}\"\n",
    "        response = requests.get(url, headers=header)\n",
    "        bs = BeautifulSoup(json.loads(response.text)[\"renderedComponent\"][\"SECTION_ARTICLE_LIST\"], 'html')\n",
    "        sleep(0.5 + randint(0, 100) * 0.1)\n",
    "\n",
    "        for element in bs.findAll(\"li\"):\n",
    "            url: str = element.select(\"a\")[0][\"href\"].strip()\n",
    "            response = requests.get(url, headers=header)\n",
    "\n",
    "            articles.append(\n",
    "                Article(\n",
    "                    title=element.select(\"strong\")[0].text.strip(),\n",
    "                    # content=element.select(\".sa_text_lede\")[0].text.strip(),\n",
    "                    content=BeautifulSoup(response.text).select(\"#newsct_article\")[0].text.strip(),\n",
    "                    url=element.select(\"a\")[0][\"href\"].strip(),\n",
    "                    pub_time=element.select(\".sa_text_datetime\")[0].text.strip(),\n",
    "                    section=section,\n",
    "                    press=element.select(\".sa_text_press\")[0].text.strip()\n",
    "                )\n",
    "            )\n",
    "\n",
    "        sleep(0.5 + randint(0, 100) * 0.1)\n",
    "\n",
    "    print(section)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "articles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "articles.pop(0)\n",
    "articles.insert(0, tuple(title for title in Article._fields))\n",
    "with open(\"./articles.csv\", 'w', encoding=\"utf8\") as f:\n",
    "    csv.writer(f).writerows(articles)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## test LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_cpp import Llama\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "model_id = 'MLP-KTLim/llama-3-Korean-Bllossom-8B-gguf-Q4_K_M'\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "model = Llama(\n",
    "    model_path='/home/gpp/src/model/llama3-korean-bllossom-8b/llama-3-Korean-Bllossom-8B-Q4_K_M.gguf',\n",
    "    n_ctx=4096,\n",
    "    n_gpu_layers=-1        # Number of model layers to offload to GPU\n",
    ")\n",
    "\n",
    "# PROMPT = \\\n",
    "# '''당신은 유용한 AI 어시스턴트입니다. 사용자의 질의에 대해 친절하고 정확하게 답변해야 합니다.\n",
    "# You are a helpful AI assistant, you'll need to answer users' queries in a friendly and accurate manner.'''\n",
    "\n",
    "# instruction = '내일 개강을 맞이하는 대학생은 무엇을 해야 합니까?'\n",
    "\n",
    "# messages = [\n",
    "#     {\"role\": \"system\", \"content\": f\"{PROMPT}\"},\n",
    "#     {\"role\": \"user\", \"content\": f\"{instruction}\"}\n",
    "#     ]\n",
    "\n",
    "# prompt = tokenizer.apply_chat_template(\n",
    "#     messages, \n",
    "#     tokenize = False,\n",
    "#     add_generation_prompt=True\n",
    "# )\n",
    "\n",
    "# generation_kwargs = {\n",
    "#     \"max_tokens\":1024,\n",
    "#     \"stop\":[\"<|eot_id|>\"],\n",
    "#     \"top_p\":0.9,\n",
    "#     \"temperature\":0.6,\n",
    "#     \"echo\":True, # Echo the prompt in the output\n",
    "# }\n",
    "\n",
    "# resonse_msg = model(prompt, **generation_kwargs)\n",
    "# print(resonse_msg['choices'][0]['text'][len(prompt):])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "response = requests.get(\"https://feeds.feedburner.com/geeknews-feed\")\n",
    "rss_feed: str = response.content.decode()\n",
    "\n",
    "print(\"RSS feed fetched successfully.\")\n",
    "\n",
    "rss_feed"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dog9",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
